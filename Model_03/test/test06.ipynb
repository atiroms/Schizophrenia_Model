{
 "nbformat": 4,
 "nbformat_minor": 2,
 "metadata": {
  "language_info": {
   "name": "python",
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "version": "3.6.6-final"
  },
  "orig_nbformat": 2,
  "file_extension": ".py",
  "mimetype": "text/x-python",
  "name": "python",
  "npconvert_exporter": "python",
  "pygments_lexer": "ipython3",
  "version": 3,
  "kernelspec": {
   "name": "python36664bittensorflowcondadaf6c54e63c544bb8ec877604aaecab0",
   "display_name": "Python 3.6.6 64-bit ('tensorflow': conda)"
  }
 },
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": "End of file.\n"
    }
   ],
   "source": [
    "######################################################################\n",
    "# Description ########################################################\n",
    "######################################################################\n",
    "'''\n",
    "Python code for meta reinforcement learning\n",
    "For a single simulation,\n",
    "  sim=Sim()\n",
    "  sim.run()\n",
    "For batch simulations,\n",
    "  batch=Batch()\n",
    "  batch.run()\n",
    "'''\n",
    "\n",
    "\n",
    "######################################################################\n",
    "# Parameters #########################################################\n",
    "######################################################################\n",
    "\n",
    "set_param_sim='param_sim.json'\n",
    "#set_param_sim='param_sim_long.json'\n",
    "#set_param_sim='param_test.json'\n",
    "#set_param_sim='param_test_load.json'\n",
    "\n",
    "set_param_mod='param_wang2018.json'\n",
    "#set_param_mod='param_wang2018_parallel.json'\n",
    "\n",
    "#dir_restart='20200219_223846'\n",
    "#dir_restart='20200221_234851'\n",
    "#dir_restart='20200222_002120'\n",
    "dir_restart=None\n",
    "\n",
    "dir_load='20200222_002120/20200222_122717'\n",
    "#dir_load=None\n",
    "\n",
    "param_batch=[\n",
    "    #{'name': 'learning_rate', 'n':11, 'type':'parametric','method':'grid','min':0.0002,'max':0.0052}\n",
    "    #{'name': 'learning_rate', 'n':10, 'type':'parametric','method':'grid','min':0.0057,'max':0.0102},\n",
    "    #{'name': 'learning_rate', 'n':100, 'type':'parametric','method':'grid','min':0.0001,'max':0.0100},\n",
    "    #{'name': 'learning_rate', 'n':2, 'type':'parametric','method':'grid','min':0.0001,'max':0.0100},\n",
    "    #{'name':'dummy_counter', 'n':3, 'type':'parametric', 'method':'grid', 'min':0,'max':2}\n",
    "    #{'name':'learning_rate', 'n':5, 'type':'parametric', 'method':'random', 'min':0.0001, 'max':0.001},\n",
    "    #{'name':'optimizer', 'n':2, 'type':'list','list':['RMSProp','Adam']}\n",
    "    #{'name':'gamma','n':3,'type':'parametric','method':'grid','min':0.7,'max':0.9}\n",
    "    #{'name': 'n_cells_lstm', 'n':20, 'type':'parametric','method':'grid','min':5,'max':100}\n",
    "    {'name': 'learning_rate', 'n':19, 'type':'parametric','method':'grid','min':0.0001,'max':0.0019},\n",
    "    #{'name': 'learning_rate', 'n':17, 'type':'parametric','method':'grid','min':0.002,'max':0.01}\n",
    "    #{'name': 'episode_stop', 'n':5, 'type':'parametric','method':'grid','min':50000,'max':0.01}\n",
    "]\n",
    "\n",
    "\n",
    "######################################################################\n",
    "# Libraries ##########################################################\n",
    "######################################################################\n",
    "\n",
    "import os\n",
    "list_path_code=[\n",
    "    'D:/atiroms/GitHub/Schizophrenia_Model/Model_03',\n",
    "    'C:/Users/atiro/GitHub/Schizophrenia_Model/Model_03',\n",
    "    '/home/atiroms/GitHub/Schizophrenia_Model/Model_03'\n",
    "]\n",
    "for i in range(len(list_path_code)):\n",
    "    if os.path.exists(list_path_code[i]):\n",
    "        path_code=list_path_code[i]\n",
    "        os.chdir(path_code)\n",
    "        break\n",
    "    elif i==len(list_path_code)-1:\n",
    "        raise ValueError('Code folder does not exist in the list.')\n",
    "list_path_save=[\n",
    "    \"/media/veracrypt1/Machine_Learning/Schizophrenia_Model/saved_data\",\n",
    "    \"/media/atiroms/MORITA_HDD3/Machine_Learning/Schizophrenia_Model/saved_data\",\n",
    "    \"C:/Users/atiro/Documents/Machine_Learning/Schizophrenia_Model/saved_data\",\n",
    "    \"D:/Machine_Learning/Schizophrenia_Model/saved_data\",\n",
    "    \"F:/Machine_Learning/Schizophrenia_Model/saved_data\"\n",
    "]\n",
    "for i in range(len(list_path_save)):\n",
    "    if os.path.exists(list_path_save[i]):\n",
    "        path_save=list_path_save[i]\n",
    "        break\n",
    "    elif i==len(list_path_save)-1:\n",
    "        raise ValueError('Save folder does not exist in the list.')\n",
    "\n",
    "import threading\n",
    "#import multiprocessing\n",
    "import numpy as np\n",
    "#import matplotlib.pyplot as plt\n",
    "import tensorflow as tf\n",
    "import datetime\n",
    "import time\n",
    "import pandas as pd\n",
    "import json\n",
    "import Agent\n",
    "import Network\n",
    "import Environment\n",
    "\n",
    "\n",
    "######################################################################\n",
    "# Parameters class for parameter exchange between classes ############\n",
    "######################################################################\n",
    "\n",
    "class Parameters():\n",
    "    def __init__(self,set_param,path_code=path_code):\n",
    "        self.set=None\n",
    "        self.add_json(set_param,path_code)\n",
    "\n",
    "    def add_dict(self,dict_param):\n",
    "        for key,value in dict_param.items():\n",
    "            if key!=\"//\":\n",
    "                setattr(self,key,value)\n",
    "\n",
    "    def add_json(self,set_param,path_code=path_code):\n",
    "        with open(os.path.join(path_code,\"parameters\",set_param)) as f:\n",
    "            dict_param=json.load(f)\n",
    "            self.add_dict(dict_param)\n",
    "        if self.set is None:\n",
    "            self.set=list()\n",
    "        self.set.append(set_param)\n",
    "\n",
    "\n",
    "######################################################################\n",
    "# Single run of simulation ###########################################\n",
    "######################################################################\n",
    "\n",
    "class Sim():\n",
    "    #def __init__(self,param_basic=param_basic,param_change=None):\n",
    "    def __init__(self,set_param_sim=set_param_sim,set_param_mod=set_param_mod,\n",
    "                 set_param_overwrite=None,\n",
    "                 path_code=path_code,path_save=path_save,\n",
    "                 path_save_batch=None,\n",
    "                 dir_load=dir_load):\n",
    "\n",
    "        # Timestamping directory name\n",
    "        datetime_start=\"{0:%Y%m%d_%H%M%S}\".format(datetime.datetime.now())\n",
    "        if path_save_batch is not None:\n",
    "            path_save_run=os.path.join(path_save_batch,datetime_start)\n",
    "        else:\n",
    "            path_save_run=os.path.join(path_save,datetime_start)\n",
    "        self.path_save=path_save\n",
    "\n",
    "        # Setup parameters in Parmeters object\n",
    "        self.param=Parameters(set_param_sim,path_code)\n",
    "        self.param.add_json(set_param_mod,path_code)\n",
    "        self.param.add_dict({'datetime_start':datetime_start, 'path_save':path_save_run})\n",
    "        if set_param_overwrite is not None:\n",
    "            self.param.add_dict(set_param_overwrite)\n",
    "        if dir_load is not None:\n",
    "            with open(os.path.join(path_save,dir_load,\"parameters.json\")) as f:\n",
    "                dict_param=json.load(f)\n",
    "            episode_done=dict_param['episode_stop']\n",
    "            episode_stop=episode_done+self.param.episode_stop\n",
    "            print('episode_stop='+str(episode_done)+'+'+str(self.param.episode_stop))\n",
    "            self.param.add_dict({'load_model':1,'dir_load':dir_load,'episode_stop':episode_stop})\n",
    "        else:\n",
    "            self.param.add_dict({'load_model':0})\n",
    "\n",
    "\n",
    "        # Make directories for saving\n",
    "        if not os.path.exists(self.param.path_save):\n",
    "            os.makedirs(self.param.path_save)\n",
    "        for subdir in ['model','pic','summary','activity']:\n",
    "            if not os.path.exists(os.path.join(self.param.path_save,subdir)):\n",
    "                os.makedirs(os.path.join(self.param.path_save,subdir))\n",
    "        \n",
    "        # Save parameters\n",
    "        with open(os.path.join(self.param.path_save,'parameters.json'), 'w') as fp:\n",
    "            json.dump(self.param.__dict__, fp, indent=1)\n",
    "\n",
    "    def run(self):\n",
    "        print('Running: '+ self.param.datetime_start + '.')\n",
    "        tf.reset_default_graph()\n",
    "        # Setup agents for multiple threading\n",
    "        with tf.device(self.param.xpu):\n",
    "            # counter of total episodes defined outside A2C_Agent class\n",
    "            self.episode_global = tf.Variable(0,dtype=tf.int32,name='episode_global',trainable=False)\n",
    "            if self.param.optimizer == \"Adam\":\n",
    "                self.trainer = tf.train.AdamOptimizer(learning_rate=self.param.learning_rate)\n",
    "            elif self.param.optimizer == \"RMSProp\":\n",
    "                self.trainer = tf.train.RMSPropOptimizer(learning_rate=self.param.learning_rate)\n",
    "            if self.param.environment == 'Two_Armed_Bandit':\n",
    "                env_alias=Environment.Two_Armed_Bandit\n",
    "            elif self.param.environment == 'Dual_Assignment_with_Hold':\n",
    "                env_alias=Environment.Dual_Assignment_with_Hold\n",
    "            # Generate master network\n",
    "            self.master_network = Network.LSTM_RNN_Network(self.param,\n",
    "                                                           env_alias(self.param.config_environment).n_actions,\n",
    "                                                           'master',None) \n",
    "            #n_agents = multiprocessing.cpu_count() # Set agents to number of available CPU threads\n",
    "            self.saver = tf.train.Saver(max_to_keep=5)\n",
    "            self.agents = []\n",
    "            # Create A2C_Agent classes (local network is defined within agent definition)\n",
    "            for i in range(self.param.n_agents):\n",
    "                self.agents.append(Agent.A2C_Agent(i,self.param,env_alias(self.param.config_environment),\n",
    "                                                   self.trainer,self.saver,self.episode_global))\n",
    "\n",
    "        # Run agents\n",
    "        #config=tf.ConfigProto(allow_soft_placement=True, log_device_placement=True)\n",
    "        config=tf.ConfigProto(allow_soft_placement=True)\n",
    "        with tf.Session(config=config) as sess:\n",
    "            sess.run(tf.global_variables_initializer())\n",
    "            if self.param.load_model == True:\n",
    "                #ckpt = tf.train.get_checkpoint_state(self.param.path_load+'/model')\n",
    "                path_load=os.path.join(self.path_save,self.param.dir_load)\n",
    "                ckpt = tf.train.get_checkpoint_state(os.path.join(path_load,'model'))\n",
    "                self.saver.restore(sess,ckpt.model_checkpoint_path)\n",
    "                print('Loaded parameters: '+ self.param.dir_load + '.')\n",
    "            #else:\n",
    "                #sess.run(tf.global_variables_initializer())\n",
    "            coord = tf.train.Coordinator()\n",
    "            #if self.param.xpu=='/gpu:0' and self.param.n_agents==1:\n",
    "            if self.param.n_agents==1:\n",
    "                self.agents[0].work(sess,coord)\n",
    "            elif self.param.xpu=='/gpu:0' and self.param.n_agents>1:\n",
    "                raise ValueError('Multi-threading not allowed with GPU.')\n",
    "            else:\n",
    "                agent_threads = []\n",
    "                for agent in self.agents:\n",
    "                    agent_work = lambda: agent.work(sess,coord)\n",
    "                    thread = threading.Thread(target=(agent_work))\n",
    "                    thread.start()\n",
    "                    agent_threads.append(thread)\n",
    "                coord.join(agent_threads)\n",
    "        print('Done single run: '+ self.param.datetime_start + '.')\n",
    "\n",
    "\n",
    "######################################################################\n",
    "# Batch run of simulations ###########################################\n",
    "######################################################################\n",
    "\n",
    "class Batch():\n",
    "    def __init__(self,param_batch=param_batch,path_save=path_save,\n",
    "                 dir_restart=dir_restart):\n",
    "        if dir_restart is None:\n",
    "            self.prep(param_batch=param_batch,path_save=path_save)\n",
    "        else:\n",
    "            self.prep_restart(dir_restart=dir_restart,path_save=path_save)\n",
    "\n",
    "    def prep_restart(self,dir_restart,path_save):\n",
    "        self.path_save_batch=os.path.join(path_save,dir_restart)\n",
    "        if os.path.exists(os.path.join(self.path_save_batch,\"batch_table.h5\")):\n",
    "            with pd.HDFStore(os.path.join(self.path_save_batch,\"batch_table.h5\")) as hdf:\n",
    "                self.batch_table = pd.DataFrame(hdf['batch_table'])\n",
    "            self.batch_table.loc[:,'run']=False\n",
    "            list_idx_rerun=self.batch_table.loc[self.batch_table['done']==False,:].index.values.tolist()\n",
    "            print('Unfinished runs: '+str(len(list_idx_rerun))+'.')\n",
    "            for i in list_idx_rerun:\n",
    "                sr_append=self.batch_table.loc[i,:]\n",
    "                sr_append['datetime_start']=np.NaN\n",
    "                sr_append['run']=True\n",
    "                sr_append['done']=False\n",
    "                self.batch_table=self.batch_table.append(sr_append)\n",
    "            self.batch_table=self.batch_table.reset_index(drop=True)\n",
    "            self.save_batch_table()\n",
    "            print('Done batch setup for restart.')\n",
    "        else:\n",
    "            print('dir_restart not found: '+dir_restart)\n",
    "\n",
    "    def prep(self,param_batch=param_batch,path_save=path_save):\n",
    "        self.n_param=len(param_batch)\n",
    "        # Timestamping directory name\n",
    "        datetime_start=\"{0:%Y%m%d_%H%M%S}\".format(datetime.datetime.now())\n",
    "        self.path_save_batch=os.path.join(path_save,datetime_start)\n",
    "        if not os.path.exists(self.path_save_batch):\n",
    "            os.makedirs(self.path_save_batch)\n",
    "\n",
    "        # Batch table preparation\n",
    "        batch_current_id=np.zeros((self.n_param,),dtype=np.int16) # table of ids of iteration for each parameter\n",
    "        self.batch_table=pd.DataFrame()\n",
    "        batch_count=0\n",
    "        flag_break=0        # 1: batch current id update successfull, 2: end of recursion\n",
    "        while flag_break < 2:\n",
    "            for i in range(self.n_param):\n",
    "                param=param_batch[i]\n",
    "                if param['type']=='list':\n",
    "                    self.batch_table.loc[batch_count,param['name']] = param['list'][batch_current_id[i]]\n",
    "                elif param['type']=='parametric':\n",
    "                    if param['method']=='grid':\n",
    "                        self.batch_table.loc[batch_count,param['name']] = param['min']+(param['max']-param['min'])*batch_current_id[i]/(param['n']-1)\n",
    "                    elif param['method']=='random':\n",
    "                        self.batch_table.loc[batch_count,param['name']] = np.random.uniform(low=param['min'],high=param['max'])\n",
    "                else:\n",
    "                    raise ValueError('Incorrect batch parameter type.')\n",
    "\n",
    "            param_id_level=self.n_param-1\n",
    "            flag_break=0\n",
    "            while flag_break < 1:\n",
    "                batch_current_id[param_id_level] += 1\n",
    "                if batch_current_id[param_id_level] < param_batch[param_id_level]['n']:\n",
    "                    # break updating id when within limit\n",
    "                    flag_break = 1\n",
    "                else:\n",
    "                    # reset current level to 0\n",
    "                    batch_current_id[param_id_level] = 0\n",
    "                    # move to the upper level\n",
    "                    param_id_level -= 1\n",
    "                    if param_id_level < 0:\n",
    "                        # break creating list when reached end\n",
    "                        flag_break = 2\n",
    "\n",
    "            batch_count += 1\n",
    "\n",
    "        self.batch_table.loc[:,'datetime_start']=np.NaN\n",
    "        self.batch_table.loc[:,'run']=True\n",
    "        self.batch_table.loc[:,'done']=False\n",
    "        self.save_batch_table()\n",
    "\n",
    "        with open(self.path_save_batch+'/parameters_batch.json', 'w') as fp:\n",
    "            json.dump(param_batch, fp, indent=1)\n",
    "\n",
    "        print('Done batch setup.')\n",
    "\n",
    "    def run(self):\n",
    "        batch_table_run=self.batch_table.loc[self.batch_table['run']==True,:]\n",
    "        #for i in range(len(self.batch_table)):\n",
    "        list_idx_run=batch_table_run.index.values.tolist()\n",
    "        for i in range(len(list_idx_run)):\n",
    "            idx=list_idx_run[i]\n",
    "            print('Batch simulation: ' + str(i + 1) + '/' + str(len(list_idx_run)),'.')\n",
    "            param_overwrite=self.batch_table.loc[idx,self.batch_table.columns.difference(['datetime_start','run','done'])].to_dict()\n",
    "            param_overwrite['path_save_batch']=self.path_save_batch\n",
    "            sim=Sim(path_save_batch=self.path_save_batch,set_param_overwrite=param_overwrite)\n",
    "            self.batch_table.loc[idx,'datetime_start']=sim.param.datetime_start\n",
    "            self.save_batch_table()\n",
    "            sim.run()\n",
    "            self.batch_table.loc[idx,'done']=True\n",
    "            self.save_batch_table()\n",
    "        print('Done batch simulation.')\n",
    "\n",
    "    def save_batch_table(self):\n",
    "        hdf=pd.HDFStore(self.path_save_batch+'/batch_table.h5')\n",
    "        hdf.put('batch_table',self.batch_table,format='table',append=False,data_columns=True)\n",
    "        hdf.close()\n",
    "\n",
    "print('End of file.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": "episode_stop=200000+50000\n"
    }
   ],
   "source": [
    "sim=Sim()\n",
    "\n",
    "sim.param.n_cells_lstm=50\n",
    "\n",
    "tf.reset_default_graph()\n",
    "with tf.device(sim.param.xpu):\n",
    "    \n",
    "    env_alias=Environment.Two_Armed_Bandit\n",
    "\n",
    "    sim.episode_global = tf.Variable(0,dtype=tf.int32,name='episode_global',trainable=False)\n",
    "\n",
    "    sim.master_network = Network.LSTM_RNN_Network(sim.param,\n",
    "                                                  env_alias(sim.param.config_environment).n_actions,\n",
    "                                                  'master',None) \n",
    "            \n",
    "    sim.saver = tf.train.Saver(max_to_keep=5)\n",
    "    sim.trainer = tf.train.RMSPropOptimizer(learning_rate=sim.param.learning_rate)\n",
    "\n",
    "    sim.agents = []\n",
    "    sim.agents.append(Agent.A2C_Agent(0,sim.param,env_alias(sim.param.config_environment),sim.trainer,sim.saver,sim.episode_global))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "path_parameter=os.path.join(path_save,'20200222_002120/20200222_122717/parameters.json')\n",
    "with open(path_parameter) as f:\n",
    "    dict_param=json.load(f)\n",
    "n_cells=dict_param['n_cells_lstm']\n",
    "n_actions=env_alias(sim.param.config_environment).n_actions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": "10320"
     },
     "execution_count": 88,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "path_variable=os.path.join(path_save,'20200222_002120/20200222_122717/model/variable.h5')\n",
    "with pd.HDFStore(path_variable) as hdf:\n",
    "    df_var = pd.DataFrame(hdf['variable'])\n",
    "df_var=df_var.loc[df_var['episode']==max(df_var['episode']),:]\n",
    "len(df_var)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 220,
   "metadata": {},
   "outputs": [],
   "source": [
    "ary_var=np.asarray(df_var['value'],order='c').astype('float32')\n",
    "ary_kernel,ary_bias,ary_fc0,ary_fc1=np.split(ary_var,\n",
    "                                             [(n_actions+2+n_cells)*4*n_cells,\n",
    "                                              (n_actions+3+n_cells)*4*n_cells,\n",
    "                                              ((n_actions+3+n_cells)*4+n_actions)*n_cells])\n",
    "ary_kernel=ary_kernel.reshape([4+n_cell,4*n_cell])\n",
    "ary_bias=ary_bias.reshape([4*n_cell])\n",
    "ary_fc0=ary_fc0.reshape([n_cell,2])\n",
    "ary_fc1=ary_fc1.reshape([n_cell])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 224,
   "metadata": {},
   "outputs": [],
   "source": [
    "sess=tf.Session()\n",
    "sess.run(tf.global_variables_initializer())\n",
    "vars_master = tf.get_collection(tf.GraphKeys.TRAINABLE_VARIABLES,'master')\n",
    "#vars_agent0 = tf.get_collection(tf.GraphKeys.TRAINABLE_VARIABLES,'agent_0')\n",
    "val_initialized = sess.run(vars_master)\n",
    "ary_kernel_init,ary_bias_init,ary_fc0_init,ary_fc1_init=val_initialized"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 229,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_cells_new=sim.param.n_cells_lstm\n",
    "#n_cells_new=50\n",
    "if n_cells_new<n_cells:\n",
    "    idx_del=np.arange(n_cells_new,n_cells)\n",
    "    idx_del_4=[]\n",
    "    for i in range(4):\n",
    "        idx_del_4=np.concatenate([idx_del_4,idx_del+n_cells*i])\n",
    "    \n",
    "    ary_kernel=np.delete(ary_kernel,idx_del+n_actions+2,0)\n",
    "    ary_kernel=np.delete(ary_kernel,idx_del_4,1)\n",
    "    ary_bias=np.delete(ary_bias,idx_del_4,0)\n",
    "    ary_fc0=np.delete(ary_fc0,idx_del,0)\n",
    "    ary_fc1=np.delete(ary_fc1,idx_del,0)\n",
    "elif n_cells_new>n_cells:\n",
    "    idx_ow=np.arange(n_cells)\n",
    "    idx_ow_4=[]\n",
    "    for i in range(4):\n",
    "        idx_ow_4=np.concatenate([idx_ow_4,idx_ow+n_cells_new*i])\n",
    "    idx_ow_4=idx_ow_4.astype('int64')\n",
    "    ary_kernel_init[np.concatenate([np.arange(n_actions+2),idx_ow+n_actions+2]),:][:,idx_ow_4]=ary_kernel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 228,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": "(52, 192)"
     },
     "execution_count": 228,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ary_kernel_init[np.concatenate([np.arange(n_actions+2),idx_ow+n_actions+2]),:][:,idx_ow_4].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 232,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": "array([[ 0.17478107,  0.36658743,  0.13955773, ..., -0.30500987,\n        -0.8377609 ,  1.4047496 ],\n       [ 1.8260309 , -0.14977968,  3.3351798 , ..., -0.05376303,\n        -0.3583426 , -0.2581303 ],\n       [ 0.12995254,  0.29092982,  0.25013828, ...,  0.1417551 ,\n        -0.0931776 ,  0.17620383],\n       ...,\n       [ 0.16412885,  0.18688573,  0.25903976, ..., -0.23034962,\n        -0.20982312, -0.3965033 ],\n       [-0.08965965, -0.17973563, -0.19185443, ...,  0.1911813 ,\n        -0.56123155,  0.45094624],\n       [ 0.39427224,  0.34855536,  0.22236167, ..., -0.25119653,\n        -0.1319986 ,  0.1018885 ]], dtype=float32)"
     },
     "execution_count": 232,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ary_kernel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 231,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": "array([[ 0.08465214,  0.08134119,  0.13187745, ...,  0.0044484 ,\n         0.09659958,  0.00904743],\n       [ 0.06097649, -0.06721371, -0.14511333, ...,  0.00502205,\n        -0.11208597, -0.08963274],\n       [-0.09176691,  0.13899899, -0.00884718, ...,  0.13978851,\n        -0.15157728, -0.02917983],\n       ...,\n       [-0.01647455, -0.04324282,  0.01957431, ..., -0.09864902,\n         0.03227679, -0.03520437],\n       [-0.13885826, -0.07119974,  0.02865747, ..., -0.04150796,\n        -0.08949558, -0.01221052],\n       [ 0.10034782,  0.10746241, -0.11666793, ...,  0.02765666,\n        -0.05073564, -0.12761536]], dtype=float32)"
     },
     "execution_count": 231,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ary_kernel_init"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 237,
   "metadata": {},
   "outputs": [],
   "source": [
    "ary_test=np.arange(12).reshape([3,4])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 241,
   "metadata": {},
   "outputs": [],
   "source": [
    "ary_test[[0,1],:][:,[0,1,2]]=np.arange(6,12).reshape([2,3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 242,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": "array([[ 0,  1,  2,  3],\n       [ 4,  5,  6,  7],\n       [ 8,  9, 10, 11]])"
     },
     "execution_count": 242,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ary_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 243,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": "array([[ 6,  7,  8],\n       [ 9, 10, 11]])"
     },
     "execution_count": 243,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.arange(6,12).reshape([2,3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": "array([[ 0.17478107,  0.36658743,  0.13955773, ..., -0.30500987,\n        -0.8377609 ,  1.4047496 ],\n       [ 1.8260309 , -0.14977968,  3.3351798 , ..., -0.05376303,\n        -0.3583426 , -0.2581303 ],\n       [ 0.12995254,  0.29092982,  0.25013828, ...,  0.1417551 ,\n        -0.0931776 ,  0.17620383],\n       ...,\n       [ 0.16412885,  0.18688573,  0.25903976, ..., -0.23034962,\n        -0.20982312, -0.3965033 ],\n       [-0.08965965, -0.17973563, -0.19185443, ...,  0.1911813 ,\n        -0.56123155,  0.45094624],\n       [ 0.39427224,  0.34855536,  0.22236167, ..., -0.25119653,\n        -0.1319986 ,  0.1018885 ]], dtype=float32)"
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sess.run(vars_master[0].assign(array_kernel))\n",
    "val_loaded = sess.run(vars_master)\n",
    "val_loaded[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "with tf.Session() as sess:\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    #vars_agent0 = tf.get_collection(tf.GraphKeys.TRAINABLE_VARIABLES,'agent_0')\n",
    "    val_initialized = sess.run(vars_master)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "No variables to save",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-7-bec456602730>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      6\u001b[0m     \u001b[0mpath_load\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mos\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpath_save\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;34m'20200222_002120/20200222_122717'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      7\u001b[0m     \u001b[0mckpt\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_checkpoint_state\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mos\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpath_load\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;34m'model'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 8\u001b[1;33m     \u001b[0msaver\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mtf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mSaver\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmax_to_keep\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m5\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      9\u001b[0m     \u001b[0msaver\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrestore\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msess\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mckpt\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmodel_checkpoint_path\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     10\u001b[0m     \u001b[0mval_loaded\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0msess\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mvars_master\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\tensorflow\\lib\\site-packages\\tensorflow\\python\\training\\saver.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, var_list, reshape, sharded, max_to_keep, keep_checkpoint_every_n_hours, name, restore_sequentially, saver_def, builder, defer_build, allow_empty, write_version, pad_step_number, save_relative_paths, filename)\u001b[0m\n\u001b[0;32m   1092\u001b[0m           time.time() + self._keep_checkpoint_every_n_hours * 3600)\n\u001b[0;32m   1093\u001b[0m     \u001b[1;32melif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mdefer_build\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1094\u001b[1;33m       \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbuild\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1095\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msaver_def\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1096\u001b[0m       \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_check_saver_def\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\tensorflow\\lib\\site-packages\\tensorflow\\python\\training\\saver.py\u001b[0m in \u001b[0;36mbuild\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   1104\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mcontext\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mexecuting_eagerly\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1105\u001b[0m       \u001b[1;32mraise\u001b[0m \u001b[0mRuntimeError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"Use save/restore instead of build in eager mode.\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1106\u001b[1;33m     \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_build\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_filename\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbuild_save\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbuild_restore\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1107\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1108\u001b[0m   \u001b[1;32mdef\u001b[0m \u001b[0m_build_eager\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcheckpoint_path\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbuild_save\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbuild_restore\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\tensorflow\\lib\\site-packages\\tensorflow\\python\\training\\saver.py\u001b[0m in \u001b[0;36m_build\u001b[1;34m(self, checkpoint_path, build_save, build_restore)\u001b[0m\n\u001b[0;32m   1129\u001b[0m           \u001b[1;32mreturn\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1130\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1131\u001b[1;33m           \u001b[1;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"No variables to save\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1132\u001b[0m       \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_is_empty\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mFalse\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1133\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: No variables to save"
     ]
    }
   ],
   "source": [
    "vars_master = tf.get_collection(tf.GraphKeys.TRAINABLE_VARIABLES,'master')\n",
    "tf.reset_default_graph()\n",
    "config=tf.ConfigProto(allow_soft_placement=True)\n",
    "with tf.Session(config=config) as sess:\n",
    "    #sess.run(tf.global_variables_initializer())\n",
    "    path_load=os.path.join(path_save,'20200222_002120/20200222_122717')\n",
    "    ckpt = tf.train.get_checkpoint_state(os.path.join(path_load,'model'))\n",
    "    saver=tf.train.Saver(max_to_keep=5)\n",
    "    saver.restore(sess,ckpt.model_checkpoint_path)\n",
    "    val_loaded=sess.run(vars_master)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": "(9, 20)"
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val_initialized[0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": "array([[ 0.17478107,  0.36658743,  0.13955773, ..., -0.30500987,\n        -0.8377609 ,  1.4047496 ],\n       [ 1.8260309 , -0.14977968,  3.3351798 , ..., -0.05376303,\n        -0.3583426 , -0.2581303 ],\n       [ 0.12995254,  0.29092982,  0.25013828, ...,  0.1417551 ,\n        -0.0931776 ,  0.17620383],\n       ...,\n       [ 0.16412885,  0.18688573,  0.25903976, ..., -0.23034962,\n        -0.20982312, -0.3965033 ],\n       [-0.08965965, -0.17973563, -0.19185443, ...,  0.1911813 ,\n        -0.56123155,  0.45094624],\n       [ 0.39427224,  0.34855536,  0.22236167, ..., -0.25119653,\n        -0.1319986 ,  0.1018885 ]], dtype=float32)"
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val_loaded[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": "[<tf.Variable 'master/rnn/LSTM_Cells/kernel:0' shape=(52, 192) dtype=float32_ref>,\n <tf.Variable 'master/rnn/LSTM_Cells/bias:0' shape=(192,) dtype=float32_ref>,\n <tf.Variable 'master/fully_connected/weights:0' shape=(48, 2) dtype=float32_ref>,\n <tf.Variable 'master/fully_connected_1/weights:0' shape=(48, 1) dtype=float32_ref>]"
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vars_master"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "#sess.run(tf.global_variables_initializer())\n",
    "path_load=os.path.join(sim.path_save,sim.param.dir_load)\n",
    "ckpt = tf.train.get_checkpoint_state(os.path.join(path_load,'model'))\n",
    "sim.saver.restore(sess,ckpt.model_checkpoint_path)\n",
    "sess.run(vars_master)"
   ]
  }
 ]
}